% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/BMTrees_prediction.R
\name{BMLMM_prediction}
\alias{BMLMM_prediction}
\title{Bayesian Mixed Linear Models for Predicting Longitudinal Outcomes with DP Priors}
\usage{
BMLMM_prediction(
  X_train,
  Y_train,
  Z_train,
  subject_id_train,
  X_test,
  Z_test,
  subject_id_test,
  model = c("BMTrees", "BMTrees_R", "BMTrees_RE", "mixedBART"),
  binary = FALSE,
  nburn = 3000L,
  npost = 4000L,
  skip = 1L,
  verbose = TRUE,
  seed = NULL,
  tol = 1e-20,
  add_intercept = TRUE
)
}
\arguments{
\item{X_train}{A matrix of covariates in the training set.}

\item{Y_train}{A numeric or logical vector of outcomes in the training set.}

\item{Z_train}{A matrix of random predictors in the training set.}

\item{subject_id_train}{A character vector of subject IDs in the training set.}

\item{X_test}{A matrix of covariates in the testing set.}

\item{Z_test}{A matrix of random predictors in the testing set.}

\item{subject_id_test}{A character vector of subject IDs in the testing set.}

\item{model}{A character string specifying the distribution assumptions for residuals and random effects.
Options are:
\itemize{
\item \code{"BMTrees"} (default): DP priors for both residuals and random effects.
\item \code{"BMTrees_R"}: DP prior for residuals, Normal prior for random effects.
\item \code{"BMTrees_RE"}: Normal prior for residuals, DP prior for random effects.
\item \code{"mixedBART"}: Normal priors for both residuals and random effects.
}}

\item{binary}{Logical. Indicates whether the outcome is binary (\code{TRUE}) or continuous (\code{FALSE}).
Default: \code{FALSE}.}

\item{nburn}{An integer specifying the number of burn-in iterations for the Gibbs sampler.
Default: \code{3000L}.}

\item{npost}{An integer specifying the number of posterior samples to collect. Default: \code{4000L}.}

\item{skip}{An integer indicating the thinning interval for MCMC samples. Default: \code{1L}.}

\item{verbose}{Logical. If \code{TRUE}, displays MCMC progress. If \code{FALSE}, shows a progress bar.
Default: \code{TRUE}.}

\item{seed}{An optional integer for setting the random seed to ensure reproducibility. Default: \code{NULL}.}

\item{tol}{A numeric tolerance value to prevent numerical overflow and underflow in the model. Default: \code{1e-20}.}

\item{add_intercept}{Logical. If \code{TRUE}, adds a column of ones (intercept) to the covariate matrices
\code{X_train} and \code{X_test}. Default: \code{TRUE}.}
}
\value{
A list containing posterior samples and predictions:
\describe{
\item{post_beta}{Posterior samples of the regression coefficients (fixed effects).}
\item{post_lmm_train}{Posterior samples of the fixed-effects predictions (\eqn{X \beta}) on training data.}
\item{post_Sigma}{Posterior samples of covariance matrices in random effects.}
\item{post_lambda_G}{Posterior samples of lambda parameter in DP normal mixture on random errors.}
\item{post_lambda_F}{Posterior samples of lambda parameter in DP normal mixture on random-effects.}
\item{post_B}{Posterior samples of the coefficients in random effects.}
\item{post_random_effect_train}{Posterior samples of random effects for training data.}
\item{post_sigma}{Posterior samples of error deviation.}
\item{post_expectation_y_train}{Posterior expectations of training data outcomes, equal to fixed-effects + random effects.}
\item{post_expectation_y_test}{Posterior expectations of testing data outcomes, equal to fixed-effects + random effects.}
\item{post_predictive_y_train}{Posterior predictive distributions for training outcomes, equal to fixed-effects + random effects + predictive residual.}
\item{post_predictive_y_test}{Posterior predictive distributions for testing outcomes, equal to fixed-effects + random effects + predictive residual.}
\item{post_eta}{Posterior samples of location parameters in DP normal mixture on random errors.}
\item{post_mu}{Posterior samples of location parameters in DP normal mixture on random effects.}
}
}
\description{
Provides predictions for outcomes in longitudinal data using Bayesian Mixed
Linear Models (BMLMM). Unlike the tree-based variant, this function assumes a linear relationship
for fixed effects while maintaining the flexible centralized Dirichlet Process (DP) framework for
random effects and residuals. It predicts values for test data while accounting for complex error structures.
}
\note{
This function utilizes modified C++ code originally derived from the
BART3 package (Bayesian Additive Regression Trees). The original package
was developed by Rodney Sparapani and is licensed
under GPL-2. Modifications were made by Jungang Zou, 2024.
}
\examples{
data <- simulation_prediction_conti(
   train_prop = 0.7,
   n_subject = 20,
   seed = 1,
   nonlinear = FALSE,
   residual = "normal",
   randeff = "MVN"
)
model <- BMLMM_prediction(
   X_train = data$X_train,
   Y_train = data$Y_train,
   Z_train = data$Z_train,
   subject_id_train = data$subject_id_train,
   X_test = data$X_test,
   Z_test = data$Z_test,
   subject_id_test = data$subject_id_test,
   model = "BMTrees",
   binary = FALSE,
   nburn = 0L, npost = 1L, skip = 1L, verbose = FALSE, seed = 1
)
}
\references{
For more information about the original BART3 package, see:
https://github.com/rsparapa/bnptools/tree/master/BART3
}
